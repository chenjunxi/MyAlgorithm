



安全智能大数据：数据湖、模型开发、数据应用、报表平台

数据入库--》数据存储--》数据计算--》数据分析和应用





# 主要工作：

![image-20220712215611270](D:\笔记\自学笔记\工作经验.assets\image-20220712215611270.png)







# 自我介绍

面试官您好，我叫陈俊茜，16年本科毕业，专业是电子信息工程，毕业后前期做了两年多的运维岗位相关的工作、19年转到大数据开发、也是我目前的工作岗位。

在大数据岗位做了四年，熟悉大数据生态，对于底层数据的治理和海量数据处理都有丰富的经验。

我们项目主要业务是和安全相关的，所以我们底层存储的对象主要是安全相关的日志，如我们机器上的Hacc日志、SACC、spes、各种防病毒日志、补丁日志等。

我们最根本项目就是大数据湖，通过入库平台创建日志收集任务，把各种数据存放在一个分布式文件系统中。这就是我们日志收集和数据存储工作。

然后基于大数据湖之上，通过分布式计算引擎对海量数据进行分层处理，进行多维度建模，为数据分析层提供数据。包括项目有资源治理、态势感知、我的安全。也些就是我们基于大数据湖的业务项目。

所以我们整体项目可以分为：数据收集、数据存储、数据计算，主要就是为数据分析服务的。

而我目前的工作也是主要负责这三项，负责大数据湖的数据治理管理工作、负责基于大数据湖之上的业务项目框架设计、搭建、数据的开发建模到测试维护一条龙服务。

从技术角度来看我们项目的技术架构由下到上分别为：

日志收集架构有：flume、logstash

数据存储层上涉及有：hdfs分布式存储文件系统、hbase非关系型数据库

数据传输层：kafka

资源调度上有：yarn

计算层有：spark mapreduce

业务模型任务管理主要是通过emr平台

数据分析层有：qpl



# 工作的问题

大数据数据开发瓶颈问题，肯定就是数据计算变慢。

问题的体现一般都在计算层展现，根因一般由上往下定位，根因基本都是底层的原因。

我们上梁不正下梁歪 

基础不牢地动山摇 **根基稳固人贤楼高**

影响最大的问题往往都是底层原因。



## 入库的问题

### 小文件

大量小文件，hdfs分布式文件系统，本身缺点就是无法高效的对大量小文件进行存储

带来的问题：

1.小则影响当前任务处理时间，小文件存储的寻址时间会超过读取时间，一万个小文件的寻址时间和百万小文件的寻址时间相差巨大，生产中出现前期准备高达半个小时的时间(正常都是几十秒）

2.大则影响整个集群性能，NameNode大量的内存来存储文件目录和块信息，庞大的文件数量会增大namenode访问压力，提高RPC的延时，延迟由毫秒级别上升到秒级别，极大严重影响大数据任务，导致整个集群性能断崖式下降。



造成小文件过多的原因：

1.数据冗余过多，一种日志，多份存储，而已都是大日子

2.原因入库模型很多是流模型和短批次写入，流模型为了实时性，写入湖的操作频繁，



解决方法：

根因：控制源头的输出文件数量，比如有些spark模型一分钟写入数据，改为五分钟。对于数据量大的2000条数据写入一次改为4w条数据写入一次。删掉冗余数据。

临时方法：就是使用spark模型对历史数据进行合并。

方法二：引进新技术，使用hudi入库，hudi可以有效控制小文件大小



加入一些自动化管理，比如shell脚本定时检查日志组件是否存活、定时清理历史文件，避免磁盘爆满。磁盘爆满问题经常有、都是告警触发才去人工处理。



### 日志监控问题：

背景就是日志多，入手少，日志出现中断，感知时间久，往往是用户反馈才知道。

常规的方法就是统计一天的数据量，方法效率低并且延迟大、消耗资源。

完美解决方法：读取hdfs最新的文件，用少量的资源和由小时级别的延迟降低到一两分钟。



## 业务模型整体的问题：

业务模型前期架构都是通过一个识别模型把数据写入hbase中，后期通过SQL对hbase数据进行分析处理。

框架：推送模型-->识别模型+缓存模型-->hbase-->SQL统计分析-->hive mongo



### 数据计算堵塞

由于ods层数据量多，容易出现大量数重复计算。

直接读取源数据然后和hbase数据进行融合计算，导致数据经常出现数据堵塞问题。

原因：

1.大量数据重复计算，一个标签作为一个数据源去识别模型进行计算。比如说计算一台机器是防病毒是否安装标签，我们防病毒有sep、McAfee、卡巴斯基、奇安信等，前期是通常的做法就是创建四个模型拉取这四种日志，进行数据去重，然后通过识别模型进行融合计算写入hbase。其实完全可以通过一个模型同时读取四种日志，进行去重合并，然后再发送去识别模型进行计算。

再进行优化我们还可以进行把标签计算类似的，全部在推送模型进行数据提取融合，再推给识别模型计算。

这个思想我源于大数据计算中有个优化叫预处理，以及大数据分层思想



通过数据分层进行预处理，这样就会减少数据计算，大大提高数据计算速度。

### 前期hbase的rowkey设计不合理导致。

问题定位过程：

spark批模型有段时间经常运行识别，发现是数据堵塞导致。经过排查在hbase的监控发现，某个表在某段时刻的读操作高度一亿，然后根据时间段排查识别模型。然后发现我们是缓存模型，有些一个机器名对应高达多个rowkey，如果全部机器参与计算，大概估算确实有一亿次读操作。

我们基于hbase数据库存放中间数据的，由于rowkey设计不够唯一性导致hbase无用的历史数据随着时间增长越来越多，也极大拖垮计算，进行了很多无用的计算。

临时方法：根据业务经验删除不必要的历史数据。定时定期删除历史数据。

根因方法：除非重构



### 常见小问题

1.负载不均衡。如topic分区设计不合理、hbase建表的时候没有进行预分区

2.不规范的代码开发。如设计代码的时候考虑数据倾斜问题、没有常见的优化

3.数据重复计算。如一些没用的字段，我们hbase高达300多个字段，其实有用就几十个。

4.表的元数据缺乏管理



### 无法解决的问题

orc格式的数据，在sparksql查询无法查到数据，同样的SQL在Hive可以查询到……







### 大数据架构上整体的问题：

1.数据没有进行数据分层，导致用户层直接访问ods层数据，导致大量重复的数据进行了重复的计算。即影响自身的速度，也影响整个集群的性能。

2.不规范的表设计，造成大表极其的大。因为数据计算的时候，会把无关的数据进行扫描。

3.保持小文件健康，减少数据量重复计算，保持正常的并行度计算



## 优化思想

数据是否有重复计算，解决这个问题才能充分发挥大数据的性能



# 大数据组件

## Hadoop

hadoop是大数据最底层的架构，主要解决是海量数据的存储和海量数据的计算问题。

hadoop包含：hdfs、MapRdeuce、yarn

### HDFS

hdfs是大数据主流的分布式文件系统。场景用于：一次写入多次读取，文件内容不支持修改。

优点：高容错性、可存放PB级别的数据、可构建在普通的机器上。

缺点：不适合大量小文件存储，不支持随机修改、不支持并发写入

![img](https://img-blog.csdnimg.cn/e497215655364581ad49981505d48a57.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Liq5YaZ5rm_55qE56iL5bqP54y_,size_11,color_FFFFFF,t_70,g_se,x_16)

HDFS高可用框架

![img](https://img2020.cnblogs.com/i-beta/1724639/202003/1724639-20200307133231451-877500874.png)

3台jnode共享编辑日志



写流程

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200721225548360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNzI3MDk1,size_16,color_FFFFFF,t_70)



读流程

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200721230022121.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNzI3MDk1,size_16,color_FFFFFF,t_70)

secondarynamenode

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200722224540440.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNzI3MDk1,size_16,color_FFFFFF,t_70)

### MapReduce

作为大数据第一代的分布式计算引擎。

良好的扩展性、高容错性、用来处理离线数据

核心思想：Map任务和Reduce任务

以进程方式处理数据：Map进程、Reduce进程

#### Maptask机制

首先是read读取文件，这个阶段会把数据生成key/value个数，并且对文件进行切割

然后map阶段：map阶段主要是自定义map的代码功能，生成新的key/value个数。

map阶段之后就是收集阶段：数据处理完最终结果写入一个环形内存缓冲区。

**溢写**阶段：当缓冲区满了之后，会将数据写入磁盘的临时文件，落盘之前对数据进行了一次排序，排序方式是按分区号排序，再按key进行排序，有必要的话数据还可以进行压缩。

合并阶段：将上述多个临时文件合并为一个大文件，即一个map任务最终生成一个文件。



#### Reducetask机制：

首先copy阶段：从maptask中读取数据到内存中，如果数据过大，就会把数据落地磁盘中

合并阶段：在远程拷贝数据的同时，后天启动两个线程对内存和磁盘中的数据进行合并

排序阶段：按key进行排序分组，reduce读取每个map中的数据，其实map来的数据本身已经排序过了，所以使用归并排序算法对所有数据进行排序即可。

reduce阶段：结果输出



上面只是一个功能的流程，如果SQL复杂的话，会启动多个mapreduce任务串行执行。



#### shuffle

map方法和reduce直接达到过程称为shuffle

包含：收集阶段、**溢写**阶段、合并阶段



### 创建问题和优化

小文件问题

优化选择高效的列式存储格式，和高效的压缩算法



## Hive

hive是数仓工具，把SQL语句解析转化成底层Mr任务，不存储数据，不用计算，不用调度任务。

hive常见优化就是小文件太多的时候，开启小文件合并参数。表的存储和压缩设计好。尽量减少不要的字段和过滤不必要的数据。

hive本身就默认优化好，如mapjoin，胃词下推，列剪裁分区剪裁，cbo优化等等

合理调节reduce数量



## HBASE

HBASE的错误使用场景，HBASE是写快读慢的数据库，分析过数据，发现我们正好是读远远大于写。

HBASE写方面，性能本身很高，注意点写的时候是否负载均衡，这和表分区有关。然后进行批量写入性能更高。是否需要wal。

value不宜过大，过大会导致频繁flush文件到磁盘中，导致storefile过多，小文件过多就引起合并操作，影响性能

影响HBASE性能主要是小文件过多，flush频繁，导致文件过多，导致索引检索时间过长。还有文件合并的时候影响性能。如果数据量急剧突增，容易导致类似于Java的stw停顿



## Spark

spark一款分布式、可扩展、基于内存的计算引擎。

官网说速度比MapReduce快100倍。

核心模块：sparkcore spark SQL sparkStreaming

Hadoop的MR框架 和 Spark框架都是数据处理框架，如何选择呢？

架构方面：

Hadoop：是分布式管理、存储、计算的生态系统；包括HDFS（存储）、MapReduce（计算）、Yarn（资源调度）
Spark：是分布式计算平台，是一个用scala语言编写的计算框架，基于内存的快速、通用、可扩展的大数据分析引擎
数据处理方面：

Hadoop：MapReduce计算模型较为单一（只有 mapper 和 reducer ），多任务之间，数据会进行落盘，不太依赖内存，适合大规模数据集的批量处理
Spark：计算模型更加丰富，采用RDD计算模型，DAG有向无环图，多任务之间，数据会在内存中处理，合适迭代式、流式计算
运行模式方面：

Hadoop：Task采用创建新的进程的方式，启动时间较慢
Spark：Task采用fork线程的方式，启动时间较快
数据通信方面：

Hadoop：多个MR作业之间数据通信，是基于磁盘
Spark：多个作业之间数据通信，是基于内存，只有在shuffle的时候会将数据写入磁盘
小结：
  Spark 确实会比 MapReduce 更有优势。但是Spark是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致Job执行失败，此时，MapReduce其实是一个更好的选择。

### 架构

标准的master-salve 一主多从架构

![image-20230618104325554](D:\笔记\自学笔记\工作经验.assets\image-20230618104325554.png)

driver

将用户程序转化为作业（job） 

 在 Executor 之间调度任务(task) 

 跟踪 Executor 的执行情况 

通过 UI 展示查询运行情况



主要两条线：初始化、申请资源、分配任务、接受最终结果

然后根据代码将数据处理流程形成一个DAG有向无环图，根据DAG图进行任务划分，然后把任务划分分别调度给计算节点。

也就是excutore任务，在计算节点创建一个jvm进程，excutore就开始处理对应的数据。

这样就达到分布式并行处理数据。

任务调度策略是FIFO。



宽依赖：一对多

shuffle过程就是将相同key的数据汇聚到一起，在这过程会涉及到磁盘IO操作，所以shuffle比较影响性能的表现。

sparkshuffle优于mapreduce就是落盘次数少。



### RDD：

弹性分布式数据集，spark上最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变的、可分区的、可并行计算的集合。

弹性：

存储上弹性、内存和磁盘自动切换

容错性：数据丢失可以根据RDD血缘关系自动恢复

计算出现异常可以重试

分布式：数据存储在大数据集群的不同节点

数据集：RDD封装了计算逻辑，并不保存数据

数据抽象：RDD是一个抽象类，需要子类具体实现

不可变：RDD封装了计算逻辑是不可变的，不同的RDD对应有不同的计算逻辑



## flume

![image-20230618173726914](D:\笔记\自学笔记\工作经验.assets\image-20230618173726914.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020080913133231.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNzI3MDk1,size_16,color_FFFFFF,t_70)





# 大数据优化总结



本人多年的大数据经验优化方面总结了几点：

## 1.数仓要分层

分层 必须分层处理数据。

- 分层主要思想是减少数据重复计算，业务上经常出现同一块数据多人使用
- 另一个原因是大数据面对亿级别的数据量，不合适直接拿来做复杂的计算和分析



## 2.大数据底层存储小文件处理

### 产生小文件原因：

- spark流数据写入数据，分钟级别写入数据；可以增加读取数据时长，写入数据可以分区
- sql写入的数据，可以控制大小

### 小文件的危害：

- 一个数据块占用内存150字节，一个小文件占用300字节，1个亿占用30G内存
- 读取一个表的时候，如果存在大量小文件的时候，会增加namenode的访问负载，导致RPC时间过长，不但影响当前任务性能，任务有可能启动不了，更严重的是，影响其他任务的性能，引起其他任务数据堵塞

### 解决小文件方案：

1.在源头进行根本性控制文件输出

2.删除冗余数据

3.使用模型合并历史小文件

4.引入hudi来控制小文件的大小



## 3.数据底层存储格式

- 数据量大的表要做到分区和分表，也是符合减少重复计算思想
- 使用orc parqurt高性能列式存储，使用高效率的lzo、snappy压缩



## 4.增加并行度

前面所说的优化其实都是针对hdfs的优化

kafka分区和hbase预分区都是直接影响任务的并行度



## 5.业务逻辑优化

- 减少数据量，过滤不必要的数据
- 执行数据的时候，最好指定我们想要的字段，因为底层一般都是列式存储方式



代码优化带来的效果很有限的，即使做到极致的优化，底层数据不好，也是白做。

由于接口人无所作为，我作为干活的人，尽量通过其他手段进行优化：如数据预处理思想、寻找数据链中存在木桶效应的、查到哪个环节存在数据重复计算，还有就是很明确需求，越清楚需求，也可以尽可能过滤不必要的计算数据。



一个问题无法从根因上彻底解决，时间一长，项目必定废掉。



## 6.SQL上优化（待补充）

其实到这点上的优化作用已经很少了，因为hive默认开启很多优化，如：mapjoin功能，预聚合功能

SQL语法上能做出的优化性价比没前面几种大了，因为SQL在解析成MR任务过程已经经历逻辑优化和物理优化了。

我们要注意的是，不要写些低级错误就行，如去重统计使用count(distinct),全局排序等操作，因为这些操作reduce阶段只有一个任务在执行。

更多的注意是业务数据的过滤。



## 7.spark优化（待补充）

- 使用广播变量，一份数据多个线程共享，解决shuffle影响
- 使用缓存，一种空间换取时间，也是解决shuffle影响
- 尽量不要使用group by这转换算子
- 减少重复计算
- cache缓存